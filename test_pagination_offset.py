#!/usr/bin/env python3
"""
æµ‹è¯•åˆ©æ¶¦è¡¨VIPæ¥å£çš„åˆ†é¡µé€»è¾‘å’Œoffsetè®¾ç½®

éªŒè¯offset=9000æ—¶æ˜¯å¦è¿˜æœ‰é‡å¤è¡Œï¼Œä»¥åŠå®Œæ•´çš„åˆ†é¡µè·å–æµç¨‹
"""

import sys
import time
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir
sys.path.insert(0, str(project_root / "src"))

from caiyuangungun.data.raw.client import get_tushare_data

def test_single_offset(offset, start_date, end_date, max_retries=3):
    """
    æµ‹è¯•å•ä¸ªoffsetå€¼çš„APIè°ƒç”¨
    
    Args:
        offset: åç§»é‡
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ  
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        tuple: (DataFrame, success_flag, error_message)
    """
    for attempt in range(max_retries):
        try:
            print(f"    ğŸ“¡ å°è¯• {attempt + 1}/{max_retries}: offset={offset}")
            
            df = get_tushare_data(
                'income_vip', 
                start_date=start_date, 
                end_date=end_date,
                offset=str(offset)
            )
            
            print(f"    âœ… æˆåŠŸè·å– {len(df)} æ¡è®°å½•")
            return df, True, None
            
        except Exception as e:
            error_msg = str(e)
            print(f"    âŒ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥:")
            print(f"       é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"       é”™è¯¯ä¿¡æ¯: {error_msg}")
            
            if attempt < max_retries - 1:
                # å¦‚æœæ˜¯429é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                if "429" in error_msg or "å¤šå°è®¾å¤‡" in error_msg:
                    wait_time = 30  # ç­‰å¾…30ç§’
                    print(f"    â³ æ£€æµ‹åˆ°é™æµé”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                else:
                    wait_time = 3
                    print(f"    â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"    ğŸš« è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ")
                return pd.DataFrame(), False, error_msg
    
    return pd.DataFrame(), False, "æœªçŸ¥é”™è¯¯"

def test_pagination_logic():
    """æµ‹è¯•åˆ†é¡µé€»è¾‘ - åŠ¨æ€è®¡ç®—offset"""
    print("ğŸ§ª æµ‹è¯•åˆ©æ¶¦è¡¨VIPæ¥å£åˆ†é¡µé€»è¾‘ï¼ˆåŠ¨æ€offsetï¼‰")
    print("=" * 60)
    
    # æµ‹è¯•å‚æ•°
    start_date = '20240401'
    end_date = '20240430'
    max_pages = 5
    overlap_size = 100  # é‡å è¡Œæ•°
    
    all_data = []
    success_count = 0
    current_offset = 0
    
    print(f"ğŸ“… æµ‹è¯•æ—¥æœŸèŒƒå›´: {start_date} - {end_date}")
    print(f"ğŸ”¢ æœ€å¤§é¡µæ•°: {max_pages}")
    print(f"ğŸ”„ é‡å è¡Œæ•°: {overlap_size}")
    print()
    
    for page_num in range(1, max_pages + 1):
        print(f"--- æµ‹è¯•ç¬¬ {page_num} é¡µ (offset={current_offset}) ---")
        
        df, success, error = test_single_offset(current_offset, start_date, end_date)
        
        if success and len(df) > 0:
            # æ·»åŠ åˆ†é¡µå…ƒæ•°æ®
            df['_offset'] = current_offset
            df['_page'] = page_num
            df['_record_count'] = len(df)
            all_data.append(df)
            success_count += 1
            
            print(f"    ğŸ“Š æ•°æ®è¯¦æƒ…:")
            print(f"       è®°å½•æ•°: {len(df)}")
            print(f"       è‚¡ç¥¨æ•°: {df['ts_code'].nunique()}")
            print(f"       å…¬å‘Šæ—¥æœŸèŒƒå›´: {df['ann_date'].min()} - {df['ann_date'].max()}")
            
            # è®¡ç®—ä¸‹ä¸€é¡µçš„offset
            next_offset = current_offset + len(df) - overlap_size
            print(f"    ğŸ“ ä¸‹ä¸€é¡µoffsetè®¡ç®—: {current_offset} + {len(df)} - {overlap_size} = {next_offset}")
            
            # å¦‚æœè¿™é¡µæ•°æ®å°‘äº4000ï¼Œè¯´æ˜å¯èƒ½æ˜¯æœ€åä¸€é¡µ
            if len(df) < 4000:
                print(f"    ğŸ æœ¬é¡µæ•°æ®é‡ < 4000ï¼Œå¯èƒ½æ˜¯æœ€åä¸€é¡µ")
                break
            
            current_offset = next_offset
            
        else:
            print(f"    âš ï¸  æ— æ•°æ®æˆ–è·å–å¤±è´¥")
            if error:
                print(f"       æœ€ç»ˆé”™è¯¯: {error}")
            break
        
        print()
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        
        print("ğŸ“Š åˆå¹¶æ•°æ®åˆ†æ:")
        print(f"   æ€»è®°å½•æ•°: {len(combined_df)}")
        print(f"   æˆåŠŸé¡µæ•°: {success_count}/{len(test_offsets)}")
        print(f"   è‚¡ç¥¨ä»£ç æ•°: {combined_df['ts_code'].nunique()}")
        
        # é‡å¤æ€§æ£€æŸ¥
        print(f"\nğŸ” é‡å¤æ€§åˆ†æ:")
        total_records = len(combined_df)
        unique_records = len(combined_df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date']))
        duplicate_records = total_records - unique_records
        
        print(f"   æ€»è®°å½•æ•°: {total_records}")
        print(f"   å”¯ä¸€è®°å½•æ•°: {unique_records}")
        print(f"   é‡å¤è®°å½•æ•°: {duplicate_records}")
        print(f"   é‡å¤ç‡: {duplicate_records/total_records*100:.1f}%")
        
        # æŒ‰é¡µåˆ†æé‡å¤æƒ…å†µ
        print(f"\nğŸ“„ æŒ‰é¡µé‡å¤åˆ†æ:")
        for page in sorted(combined_df['_page'].unique()):
            page_data = combined_df[combined_df['_page'] == page]
            page_offset = page_data['_offset'].iloc[0]
            
            # æ£€æŸ¥ä¸å‰é¢æ‰€æœ‰é¡µçš„é‡å¤
            previous_data = combined_df[combined_df['_page'] < page]
            if len(previous_data) > 0:
                page_keys = set(page_data['ts_code'] + '_' + page_data['ann_date'].astype(str) + '_' + page_data['end_date'].astype(str))
                prev_keys = set(previous_data['ts_code'] + '_' + previous_data['ann_date'].astype(str) + '_' + previous_data['end_date'].astype(str))
                overlap = len(page_keys.intersection(prev_keys))
                print(f"   ç¬¬{page}é¡µ (offset={page_offset}): {len(page_data)}æ¡è®°å½•, ä¸å‰é¢é¡µé‡å¤{overlap}æ¡")
            else:
                print(f"   ç¬¬{page}é¡µ (offset={page_offset}): {len(page_data)}æ¡è®°å½•, é¦–é¡µæ— é‡å¤")
        
        # ä¿å­˜ç»“æœ
        output_file = project_root / "pagination_test_result.parquet"
        combined_df.to_parquet(output_file, index=False)
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        
        # ä¿å­˜å»é‡åçš„æ•°æ®
        unique_df = combined_df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'])
        unique_output_file = project_root / "pagination_test_result_unique.parquet"
        unique_df.to_parquet(unique_output_file, index=False)
        print(f"ğŸ’¾ å»é‡æ•°æ®å·²ä¿å­˜åˆ°: {unique_output_file}")
        
        return combined_df
    else:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
        return pd.DataFrame()

if __name__ == "__main__":
    try:
        result_df = test_pagination_logic()
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
