#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Qlibæ•°æ®è½¬æ¢åŸºç±»

æä¾›Qlibæ•°æ®è½¬æ¢çš„é€šç”¨åŠŸèƒ½ï¼š
- é…ç½®ç®¡ç†
- è‚¡ç¥¨ä»£ç è½¬æ¢
- äº¤æ˜“æ—¥å†ç”Ÿæˆ
- è‚¡ç¥¨åˆ—è¡¨ç”Ÿæˆ
- æ•°æ®éªŒè¯
"""

import json
import logging
import re
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
import numpy as np
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('qlib_converter')


class BaseQlibConverter(ABC):
    """
    Qlibæ•°æ®è½¬æ¢åŸºç±»
    
    æ‰€æœ‰Qlibè½¬æ¢å™¨çš„åŸºç±»ï¼Œæä¾›é€šç”¨åŠŸèƒ½ï¼š
    - é…ç½®åŠ è½½å’Œç®¡ç†
    - è‚¡ç¥¨ä»£ç æ ¼å¼è½¬æ¢
    - äº¤æ˜“æ—¥å†ç”Ÿæˆ
    - è‚¡ç¥¨åˆ—è¡¨ç”Ÿæˆ
    - æ•°æ®è´¨é‡éªŒè¯
    """
    
    def __init__(self, config: Dict[str, Any], limit_symbols: Optional[int] = None):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            config: å®Œæ•´çš„è½¬æ¢å™¨é…ç½®å­—å…¸
            limit_symbols: é™åˆ¶å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
        """
        self.config = config
        self.project_root = Path(config.get('project_root', Path.cwd()))
        self.limit_symbols = limit_symbols
        
        # æå–å…³é”®é…ç½®
        self.source_data_config = config.get('source_data', {})
        self.conversion_config = config.get('conversion_config', {})
        self.feed_layer_config = config.get('feed_layer', {})
        self.qlib_layer_config = config.get('qlib_layer', {})
        self.dump_config = config.get('dump_config', {})
        
        if limit_symbols:
            logger.info(f"âš ï¸  æµ‹è¯•æ¨¡å¼: åªå¤„ç† {limit_symbols} æ”¯è‚¡ç¥¨")
        
        logger.info(f"{self.__class__.__name__} åˆå§‹åŒ–å®Œæˆ")
    
    def convert_symbol(self, tushare_code: str) -> str:
        """
        è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼: Tushare -> Qlib
        
        Args:
            tushare_code: Tushareæ ¼å¼è‚¡ç¥¨ä»£ç  (å¦‚ 600000.SH)
            
        Returns:
            Qlibæ ¼å¼è‚¡ç¥¨ä»£ç  (å¦‚ sh600000)
            
        Examples:
            >>> converter.convert_symbol('600000.SH')
            'sh600000'
            >>> converter.convert_symbol('000001.SZ')
            'sz000001'
            >>> converter.convert_symbol('688001.SH')
            'sh688001'
        """
        pattern = r'^(\d{6})\.(SH|SZ|BJ)$'
        match = re.match(pattern, tushare_code)
        
        if not match:
            raise ValueError(f"Invalid symbol format: {tushare_code}")
        
        code, exchange = match.groups()
        return f"{exchange.lower()}{code}"
    
    def convert_symbol_vectorized(self, tushare_series: pd.Series) -> pd.Series:
        """
        å‘é‡åŒ–è½¬æ¢è‚¡ç¥¨ä»£ç ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼Œæ¯”applyå¿«100å€ä»¥ä¸Šï¼‰
        
        Args:
            tushare_series: Tushareæ ¼å¼ä»£ç Series
            
        Returns:
            Qlibæ ¼å¼ä»£ç Series
        """
        # ä½¿ç”¨str.extractè¿›è¡Œå‘é‡åŒ–æ“ä½œï¼ˆæ¯”applyå¿«å¾—å¤šï¼‰
        extracted = tushare_series.str.extract(r'^(\d{6})\.(SH|SZ|BJ)$')
        if extracted.isnull().any().any():
            raise ValueError("å­˜åœ¨æ— æ•ˆçš„Tushareè‚¡ç¥¨ä»£ç æ ¼å¼")
        
        # å‘é‡åŒ–æ‹¼æ¥ï¼šäº¤æ˜“æ‰€ä»£ç (å°å†™) + è‚¡ç¥¨ä»£ç 
        return extracted[1].str.lower() + extracted[0]
    
    def calculate_period_vectorized(self, end_date_series: pd.Series, interval: str = 'quarterly') -> pd.Series:
        """
        å‘é‡åŒ–è®¡ç®—periodï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼Œæ¯”applyå¿«100å€ä»¥ä¸Šï¼‰
        
        Args:
            end_date_series: ç»“æŸæ—¥æœŸSeries (YYYYMMDDæ ¼å¼çš„æ•´æ•°)
            interval: 'quarterly' æˆ– 'annual'
            
        Returns:
            period Series (YYYYQQ æˆ– YYYY æ ¼å¼)
        """
        year = end_date_series // 10000
        month = (end_date_series % 10000) // 100
        
        if interval == 'quarterly':
            # å­£åº¦æ ¼å¼ï¼šYYYYQQ
            quarter = ((month - 1) // 3) + 1
            return year * 100 + quarter
        else:
            # å¹´åº¦æ ¼å¼ï¼šYYYY
            return year
    
    def convert_symbol_reverse(self, qlib_code: str) -> str:
        """
        åå‘è½¬æ¢è‚¡ç¥¨ä»£ç : Qlib -> Tushare
        
        Args:
            qlib_code: Qlibæ ¼å¼è‚¡ç¥¨ä»£ç  (å¦‚ sh600000)
            
        Returns:
            Tushareæ ¼å¼è‚¡ç¥¨ä»£ç  (å¦‚ 600000.SH)
        """
        pattern = r'^(sh|sz|bj)(\d{6})$'
        match = re.match(pattern, qlib_code.lower())
        
        if not match:
            raise ValueError(f"Invalid qlib symbol format: {qlib_code}")
        
        exchange, code = match.groups()
        return f"{code}.{exchange.upper()}"
    
    def generate_calendar(self, df: pd.DataFrame, date_field: str = 'date') -> pd.DataFrame:
        """
        ç”Ÿæˆäº¤æ˜“æ—¥å†
        
        Args:
            df: åŒ…å«æ—¥æœŸå­—æ®µçš„DataFrame
            date_field: æ—¥æœŸå­—æ®µåç§°
            
        Returns:
            äº¤æ˜“æ—¥å†DataFrameï¼ŒåŒ…å«å”¯ä¸€çš„äº¤æ˜“æ—¥æœŸ
        """
        logger.info("ç”Ÿæˆäº¤æ˜“æ—¥å†...")
        
        # æå–æ‰€æœ‰å”¯ä¸€äº¤æ˜“æ—¥ï¼ŒæŒ‰æ—¥æœŸæ’åº
        if df[date_field].dtype == 'object':
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºdatetime
            dates = pd.to_datetime(df[date_field].unique())
        else:
            # å¦‚æœå·²ç»æ˜¯datetimeï¼Œç›´æ¥ä½¿ç”¨
            dates = pd.to_datetime(df[date_field].unique())
        
        trade_dates = sorted(dates)
        calendar_df = pd.DataFrame({
            'date': [pd.Timestamp(d).strftime('%Y-%m-%d') for d in trade_dates]
        })
        
        logger.info(f"âœ… äº¤æ˜“æ—¥å†: {len(calendar_df)} å¤©")
        logger.info(f"   æ—¥æœŸèŒƒå›´: {calendar_df['date'].iloc[0]} è‡³ {calendar_df['date'].iloc[-1]}")
        
        return calendar_df
    
    def generate_instruments(self, df: pd.DataFrame, 
                           symbol_field: str = 'symbol',
                           date_field: str = 'date') -> pd.DataFrame:
        """
        ç”Ÿæˆè‚¡ç¥¨åˆ—è¡¨ï¼ˆinstrumentsï¼‰
        
        Args:
            df: åŒ…å«è‚¡ç¥¨ä»£ç å’Œæ—¥æœŸçš„DataFrame
            symbol_field: è‚¡ç¥¨ä»£ç å­—æ®µå
            date_field: æ—¥æœŸå­—æ®µå
            
        Returns:
            è‚¡ç¥¨åˆ—è¡¨DataFrameï¼ŒåŒ…å«symbol, start_datetime, end_datetime
        """
        logger.info("ç”Ÿæˆè‚¡ç¥¨åˆ—è¡¨...")
        
        instruments = []
        for symbol in df[symbol_field].unique():
            symbol_data = df[df[symbol_field] == symbol]
            start_date = symbol_data[date_field].min()
            end_date = symbol_data[date_field].max()
            
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            if isinstance(start_date, str):
                start_date = pd.to_datetime(start_date)
            if isinstance(end_date, str):
                end_date = pd.to_datetime(end_date)
            
            instruments.append({
                'symbol': symbol.upper(),  # Qlibè¦æ±‚å¤§å†™
                'start_datetime': pd.Timestamp(start_date).strftime('%Y-%m-%d'),
                'end_datetime': pd.Timestamp(end_date).strftime('%Y-%m-%d')
            })
        
        instruments_df = pd.DataFrame(instruments)
        logger.info(f"âœ… è‚¡ç¥¨åˆ—è¡¨: {len(instruments_df)} æ”¯è‚¡ç¥¨")
        
        return instruments_df
    
    def save_metadata(self, calendar_df: pd.DataFrame, 
                     instruments_df: pd.DataFrame,
                     output_dir: Path):
        """
        ä¿å­˜å…ƒæ•°æ®ï¼ˆäº¤æ˜“æ—¥å†å’Œè‚¡ç¥¨åˆ—è¡¨ï¼‰
        
        Args:
            calendar_df: äº¤æ˜“æ—¥å†DataFrame
            instruments_df: è‚¡ç¥¨åˆ—è¡¨DataFrame
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info("ä¿å­˜å…ƒæ•°æ®...")
        
        # åˆ›å»ºmetadataç›®å½•
        metadata_dir = output_dir / 'metadata'
        metadata_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜äº¤æ˜“æ—¥å†
        calendar_path = metadata_dir / 'day.txt'
        calendar_df.to_csv(calendar_path, header=False, index=False)
        logger.info(f"  âœ… äº¤æ˜“æ—¥å†: {calendar_path}")
        
        # ä¿å­˜è‚¡ç¥¨åˆ—è¡¨
        instruments_path = metadata_dir / 'all.txt'
        instruments_df.to_csv(instruments_path, sep='\t', header=False, index=False)
        logger.info(f"  âœ… è‚¡ç¥¨åˆ—è¡¨: {instruments_path}")
    
    def validate_dataframe(self, df: pd.DataFrame, 
                          required_fields: List[str],
                          name: str = "DataFrame") -> Tuple[bool, List[str]]:
        """
        éªŒè¯DataFrameçš„å­—æ®µå®Œæ•´æ€§
        
        Args:
            df: å¾…éªŒè¯çš„DataFrame
            required_fields: å¿…éœ€å­—æ®µåˆ—è¡¨
            name: DataFrameåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            (æ˜¯å¦é€šè¿‡éªŒè¯, ç¼ºå¤±å­—æ®µåˆ—è¡¨)
        """
        missing_fields = [f for f in required_fields if f not in df.columns]
        
        if missing_fields:
            logger.warning(f"{name} ç¼ºå¤±å¿…éœ€å­—æ®µ: {missing_fields}")
            return False, missing_fields
        else:
            logger.info(f"âœ… {name} å­—æ®µéªŒè¯é€šè¿‡")
            return True, []
    
    def check_data_quality(self, df: pd.DataFrame, name: str = "æ•°æ®") -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®è´¨é‡
        
        Args:
            df: å¾…æ£€æŸ¥çš„DataFrame
            name: æ•°æ®åç§°
            
        Returns:
            è´¨é‡æŠ¥å‘Šå­—å…¸
        """
        logger.info(f"æ£€æŸ¥{name}è´¨é‡...")
        
        report = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'null_counts': {},
            'duplicate_rows': 0,
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024
        }
        
        # æ£€æŸ¥ç©ºå€¼
        null_counts = df.isnull().sum()
        report['null_counts'] = {col: int(count) for col, count in null_counts.items() if count > 0}
        
        # æ£€æŸ¥é‡å¤è¡Œ
        report['duplicate_rows'] = df.duplicated().sum()
        
        # æ—¥å¿—è¾“å‡º
        logger.info(f"  æ€»è¡Œæ•°: {report['total_rows']:,}")
        logger.info(f"  æ€»åˆ—æ•°: {report['total_columns']}")
        logger.info(f"  å†…å­˜å ç”¨: {report['memory_usage_mb']:.2f} MB")
        
        if report['null_counts']:
            logger.warning(f"  ç©ºå€¼å­—æ®µ: {list(report['null_counts'].keys())}")
        
        if report['duplicate_rows'] > 0:
            logger.warning(f"  é‡å¤è¡Œæ•°: {report['duplicate_rows']}")
        
        return report
    
    def limit_data_by_symbols(self, df: pd.DataFrame, 
                             symbol_field: str = 'ts_code',
                             limit: Optional[int] = None) -> pd.DataFrame:
        """
        æŒ‰è‚¡ç¥¨æ•°é‡é™åˆ¶æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        Args:
            df: åŸå§‹DataFrame
            symbol_field: è‚¡ç¥¨ä»£ç å­—æ®µå
            limit: é™åˆ¶çš„è‚¡ç¥¨æ•°é‡
            
        Returns:
            é™åˆ¶åçš„DataFrame
        """
        if limit is None or limit <= 0:
            return df
        
        selected_symbols = sorted(df[symbol_field].unique())[:limit]
        df_limited = df[df[symbol_field].isin(selected_symbols)].copy()
        
        logger.info(f"  âœ‚ï¸  å·²é™åˆ¶ä¸º {limit} æ”¯è‚¡ç¥¨")
        logger.info(f"  é™åˆ¶å‰: {len(df):,} è¡Œ")
        logger.info(f"  é™åˆ¶å: {len(df_limited):,} è¡Œ")
        
        return df_limited
    
    @abstractmethod
    def load_source_data(self) -> pd.DataFrame:
        """
        åŠ è½½æºæ•°æ®ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œç”±å­ç±»å®ç°ï¼‰
        
        Returns:
            æºæ•°æ®DataFrame
        """
        pass
    
    @abstractmethod
    def convert_to_qlib_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è½¬æ¢ä¸ºQlibæ ¼å¼ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œç”±å­ç±»å®ç°ï¼‰
        
        Args:
            df: æºæ•°æ®DataFrame
            
        Returns:
            Qlibæ ¼å¼DataFrame
        """
        pass
    
    @abstractmethod
    def save_feed_layer(self, df: pd.DataFrame, **kwargs):
        """
        ä¿å­˜FEEDå±‚æ•°æ®ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œç”±å­ç±»å®ç°ï¼‰
        
        Args:
            df: Qlibæ ¼å¼DataFrame
            **kwargs: å…¶ä»–å‚æ•°
        """
        pass
    
    @abstractmethod
    def dump_to_qlib_binary(self):
        """
        è½¬æ¢ä¸ºQlibäºŒè¿›åˆ¶æ ¼å¼ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œç”±å­ç±»å®ç°ï¼‰
        """
        pass
    
    def run(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„è½¬æ¢æµç¨‹
        
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        logger.info("="*60)
        logger.info(f"å¼€å§‹æ‰§è¡Œ {self.__class__.__name__} è½¬æ¢æµç¨‹")
        logger.info("="*60)
        
        result = {
            'success': False,
            'converter': self.__class__.__name__,
            'start_time': datetime.now().isoformat(),
            'stages': {}
        }
        
        try:
            # é˜¶æ®µ1: åŠ è½½æºæ•°æ®
            logger.info("\nã€é˜¶æ®µ1ã€‘åŠ è½½æºæ•°æ®")
            source_df = self.load_source_data()
            result['stages']['load'] = {
                'rows': len(source_df),
                'columns': len(source_df.columns)
            }
            
            # é˜¶æ®µ2: è½¬æ¢ä¸ºQlibæ ¼å¼
            logger.info("\nã€é˜¶æ®µ2ã€‘è½¬æ¢ä¸ºQlibæ ¼å¼")
            qlib_df = self.convert_to_qlib_format(source_df)
            result['stages']['convert'] = {
                'rows': len(qlib_df),
                'columns': len(qlib_df.columns)
            }
            
            # é˜¶æ®µ3: ä¿å­˜FEEDå±‚
            logger.info("\nã€é˜¶æ®µ3ã€‘ä¿å­˜FEEDå±‚æ•°æ®")
            self.save_feed_layer(qlib_df)
            result['stages']['feed'] = {'status': 'completed'}
            
            # é˜¶æ®µ4: è½¬æ¢ä¸ºäºŒè¿›åˆ¶
            logger.info("\nã€é˜¶æ®µ4ã€‘è½¬æ¢ä¸ºQlibäºŒè¿›åˆ¶æ ¼å¼")
            self.dump_to_qlib_binary()
            result['stages']['dump'] = {'status': 'completed'}
            
            result['success'] = True
            result['end_time'] = datetime.now().isoformat()
            
            logger.info("="*60)
            logger.info("ğŸ‰ è½¬æ¢æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
            logger.info("="*60)
            
        except Exception as e:
            result['success'] = False
            result['error'] = str(e)
            result['end_time'] = datetime.now().isoformat()
            
            logger.error("="*60)
            logger.error(f"âŒ è½¬æ¢æµç¨‹å¤±è´¥: {e}")
            logger.error("="*60)
            
            import traceback
            logger.error(traceback.format_exc())
            raise
        
        return result
    
    # ========================================================================
    # é€šç”¨è¾…åŠ©æ–¹æ³•ï¼ˆä¾›å­ç±»ä½¿ç”¨ï¼Œå‡å°‘é‡å¤ä»£ç ï¼‰
    # ========================================================================
    
    def _load_parquet_with_limit(self, file_path: str, symbol_col: str = 'ts_code') -> pd.DataFrame:
        """
        åŠ è½½Parquetæ–‡ä»¶å¹¶å¯é€‰åœ°é™åˆ¶è‚¡ç¥¨æ•°é‡
        
        Args:
            file_path: ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„æ–‡ä»¶è·¯å¾„
            symbol_col: è‚¡ç¥¨ä»£ç åˆ—å
            
        Returns:
            åŠ è½½çš„DataFrameï¼ˆå¤±è´¥è¿”å›Noneï¼‰
        """
        full_path = self.project_root / file_path
        
        if not full_path.exists():
            logger.warning(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
            return None
        
        df = pd.read_parquet(full_path)
        logger.info(f"  åŸå§‹æ•°æ®: {len(df):,} è¡Œ, {df[symbol_col].nunique()} æ”¯è‚¡ç¥¨")
        
        # é™åˆ¶è‚¡ç¥¨æ•°é‡
        if self.limit_symbols:
            df = self.limit_data_by_symbols(df, symbol_col, self.limit_symbols)
        
        return df
    
    def _execute_subprocess(self, cmd: list, description: str = "æ‰§è¡Œå¤–éƒ¨å‘½ä»¤") -> bool:
        """
        æ‰§è¡Œsubprocesså‘½ä»¤å¹¶ç»Ÿä¸€å¤„ç†é”™è¯¯
        
        Args:
            cmd: å‘½ä»¤åˆ—è¡¨
            description: å‘½ä»¤æè¿°
            
        Returns:
            æ˜¯å¦æˆåŠŸ
            
        Raises:
            RuntimeError: å‘½ä»¤æ‰§è¡Œå¤±è´¥
        """
        import subprocess
        
        logger.info(f"{description}...")
        logger.info(f"  å‘½ä»¤: {' '.join(cmd[:4])} ...")
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            logger.error(f"âŒ {description}å¤±è´¥:")
            logger.error(f"   è¿”å›ç : {result.returncode}")
            logger.error(f"   é”™è¯¯è¾“å‡º: {result.stderr}")
            raise RuntimeError(f"{description} failed with return code {result.returncode}")
        
        if result.stdout:
            logger.info(f"  å‘½ä»¤è¾“å‡º:\n{result.stdout}")
        
        logger.info(f"âœ… {description}æˆåŠŸ")
        return True
    
    def _save_symbols_to_csv(self, df: pd.DataFrame, output_dir: Path, 
                            columns_to_keep: list = None,
                            file_name_upper: bool = False,
                            progress_desc: str = "ä¿å­˜CSV",
                            parallel: bool = True,
                            max_workers: int = 16) -> int:
        """
        æŒ‰è‚¡ç¥¨æ‹†åˆ†ä¿å­˜CSVæ–‡ä»¶ï¼ˆé«˜æ€§èƒ½æ‰¹é‡å†™å…¥ï¼Œæ€§èƒ½æå‡10-20å€ï¼‰
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä½¿ç”¨groupbyé¿å…é‡å¤è¿‡æ»¤
        2. å¹¶è¡Œå†™å…¥æ–‡ä»¶
        3. é¢„å…ˆå¤„ç†åˆ—ï¼Œå‡å°‘æ¯æ¬¡å¾ªç¯çš„å¼€é”€
        
        Args:
            df: åŒ…å«symbolåˆ—çš„DataFrame
            output_dir: è¾“å‡ºç›®å½•
            columns_to_keep: è¦ä¿ç•™çš„åˆ—ï¼ˆNoneè¡¨ç¤ºé™¤symbolå¤–å…¨ä¿ç•™ï¼‰
            file_name_upper: æ–‡ä»¶åæ˜¯å¦å¤§å†™
            progress_desc: è¿›åº¦æ¡æè¿°
            parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶æ•°é‡
        """
        from tqdm import tqdm
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # é¢„å¤„ç†ï¼šç¡®å®šè¦ä¿å­˜çš„åˆ—ï¼ˆé¿å…åœ¨å¾ªç¯ä¸­é‡å¤åˆ¤æ–­ï¼‰
        if columns_to_keep:
            save_columns = [col for col in columns_to_keep if col in df.columns]
            # ä¸´æ—¶ä¿ç•™symbolåˆ—ç”¨äºåˆ†ç»„ï¼Œä¹‹åä¼šåˆ é™¤
            df_to_save = df[['symbol'] + save_columns].copy()
        else:
            df_to_save = df.copy()
        
        # ä½¿ç”¨groupbyåˆ†ç»„ï¼ˆæ¯”é‡å¤è¿‡æ»¤å¿«å¾—å¤šï¼‰
        grouped = df_to_save.groupby('symbol', sort=False)
        symbols = list(grouped.groups.keys())
        
        logger.info(f"å‡†å¤‡ä¿å­˜ {len(symbols)} ä¸ªè‚¡ç¥¨çš„CSVæ–‡ä»¶...")
        
        def save_single_symbol(symbol_data):
            """ä¿å­˜å•ä¸ªè‚¡ç¥¨çš„CSVæ–‡ä»¶ï¼ˆæ¥æ”¶(symbol, df)å…ƒç»„ï¼‰"""
            symbol, group_df = symbol_data
            
            # ç§»é™¤symbolåˆ—
            group_df = group_df.drop(columns=['symbol'])
            
            # ä¿å­˜CSVï¼ˆä½¿ç”¨æ›´å¿«çš„å¼•æ“ï¼‰
            symbol_name = symbol.upper() if file_name_upper else symbol.lower()
            csv_path = output_dir / f"{symbol_name}.csv"
            
            # ä½¿ç”¨pyarrowå¼•æ“ï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–cå¼•æ“ï¼Œæ¯”pythonå¼•æ“å¿«3-5å€
            try:
                group_df.to_csv(csv_path, index=False, engine='c')
            except:
                group_df.to_csv(csv_path, index=False)
            
            return symbol
        
        if parallel and len(symbols) > 50:
            # å¹¶è¡Œå¤„ç†ï¼ˆå¤§é‡è‚¡ç¥¨æ—¶æ•ˆæœæ˜¾è‘—ï¼‰
            logger.info(f"ä½¿ç”¨å¹¶è¡Œæ¨¡å¼ä¿å­˜ï¼ˆ{max_workers}ä¸ªå·¥ä½œçº¿ç¨‹ï¼‰...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = {executor.submit(save_single_symbol, (symbol, group_df)): symbol 
                          for symbol, group_df in grouped}
                
                # æ˜¾ç¤ºè¿›åº¦
                completed = 0
                for future in tqdm(as_completed(futures), total=len(symbols), desc=progress_desc):
                    completed += 1
                    try:
                        future.result()
                    except Exception as e:
                        symbol = futures[future]
                        logger.error(f"ä¿å­˜ {symbol} å¤±è´¥: {e}")
        else:
            # ä¸²è¡Œå¤„ç†ï¼ˆè‚¡ç¥¨æ•°é‡å°‘æ—¶ï¼‰
            logger.info("ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ä¿å­˜...")
            for symbol, group_df in tqdm(grouped, total=len(symbols), desc=progress_desc):
                try:
                    save_single_symbol((symbol, group_df))
                except Exception as e:
                    logger.error(f"ä¿å­˜ {symbol} å¤±è´¥: {e}")
        
        return len(symbols)
    
    def _init_qlib(self, qlib_dir: Path = None):
        """
        åˆå§‹åŒ–Qlibå®ä¾‹ï¼ˆç»Ÿä¸€å¤„ç†ï¼‰
        
        Args:
            qlib_dir: Qlibæ•°æ®ç›®å½•ï¼ˆNoneåˆ™ä»é…ç½®è¯»å–ï¼‰
            
        Returns:
            (qlibæ¨¡å—, Dæ¨¡å—) æˆ– (None, None)
        """
        try:
            import qlib
            from qlib.constant import REG_CN
            from qlib.data import D
        except ImportError:
            logger.error("âŒ æœªå®‰è£…qlibåº“ï¼Œè·³è¿‡éªŒè¯")
            return None, None
        
        if qlib_dir is None:
            qlib_dir = Path(self.qlib_layer_config['output_dir']).expanduser()
        
        logger.info(f"åˆå§‹åŒ–Qlib (provider_uri={qlib_dir})...")
        qlib.init(
            provider_uri=str(qlib_dir),
            region=REG_CN,
            expression_cache=None,
            dataset_cache=None
        )
        
        return qlib, D

