#!/usr/bin/env python3
"""
æ•°æ®æ¸…æ´—æœåŠ¡ç¤ºä¾‹
æ”¯æŒä»»æ„æŒ‡å®šæ•°æ®æºçš„æ•°æ®æ¸…æ´—ï¼Œå‚è€ƒexample_processor_service.pyçš„è®¾è®¡æ¨¡å¼
æ”¯æŒè¿è¡ŒæŒ‡å®šæ•°æ®æºæˆ–å…¨éƒ¨å¯ç”¨æ•°æ®æºçš„æ¸…æ´—
"""

import sys
import os
import logging
from pathlib import Path
from typing import Optional, Dict, List

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('example_data_cleaning_service')

# åŠ¨æ€æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

from caiyuangungun.data.norm.services.basic_data_cleaning_service import DataCleaningService
import pandas as pd


class DataCleaningServiceExample:
    """æ•°æ®æ¸…æ´—æœåŠ¡ç¤ºä¾‹ç±»"""
    
    def __init__(self):
        self.service = DataCleaningService()
        self.enabled_sources = self._get_enabled_data_sources()
    
    def _get_enabled_data_sources(self) -> Dict[str, List[str]]:
        """
        è·å–æ‰€æœ‰enabled=trueçš„æ•°æ®æº
        
        Returns:
            Dict[str, List[str]]: æŒ‰æ•°æ®ç±»å‹åˆ†ç»„çš„å¯ç”¨æ•°æ®æº
        """
        config = self.service.config_manager.get_basic_cleaning_config()
        
        enabled_sources = {}
        
        for data_type, type_config in config.items():
            if 'cleaning_pipelines' in type_config:
                enabled_list = []
                for source_name, source_config in type_config['cleaning_pipelines'].items():
                    if source_config.get('enabled', True):  # é»˜è®¤ä¸ºTrue
                        enabled_list.append(source_name)
                
                if enabled_list:
                    enabled_sources[data_type] = enabled_list
        
        return enabled_sources
    
    def list_available_data_sources(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"""
        logger.info("=== å¯ç”¨çš„æ•°æ®æºåˆ—è¡¨ ===")
        
        if not self.enabled_sources:
            logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®æº")
            return
        
        total_sources = 0
        for data_type, sources in self.enabled_sources.items():
            logger.info(f"\nğŸ“Š {data_type}:")
            for source in sources:
                logger.info(f"  âœ“ {source}")
                total_sources += 1
        
        logger.info(f"\næ€»è®¡: {total_sources} ä¸ªæ•°æ®æº")
        return self.enabled_sources
    
    def get_data_source_info(self, data_type: str, data_source: str = None) -> Optional[Dict]:
        """è·å–æ•°æ®æºè¯¦ç»†ä¿¡æ¯"""
        if data_type not in self.enabled_sources:
            logger.error(f"æ•°æ®ç±»å‹ {data_type} ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
            return None
        
        if data_source and data_source not in self.enabled_sources[data_type]:
            logger.error(f"æ•°æ®æº {data_source} åœ¨ {data_type} ä¸­ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
            return None
        
        config = self.service.config_manager.get_basic_cleaning_config()
        type_config = config.get(data_type, {})
        
        info = {
            'data_type': data_type,
            'description': type_config.get('description', ''),
            'output_path': type_config.get('output_path', ''),
            'available_sources': self.enabled_sources[data_type]
        }
        
        if data_source:
            source_config = type_config.get('cleaning_pipelines', {}).get(data_source, {})
            info['data_source'] = data_source
            info['enabled'] = source_config.get('enabled', True)
            info['pipeline_steps'] = len(source_config.get('pipeline', []))
            info['pipeline'] = source_config.get('pipeline', [])
        
        return info
    
    def clean_specific_data_source(self, data_type: str, data_source: str, max_files: Optional[int] = None) -> bool:
        """æ¸…æ´—æŒ‡å®šçš„æ•°æ®æº"""
        logger.info(f"=== å¼€å§‹æ¸…æ´—æ•°æ®æº: {data_type}/{data_source} ===")
        
        # æ£€æŸ¥æ•°æ®æºæ˜¯å¦å­˜åœ¨
        if data_type not in self.enabled_sources:
            logger.error(f"âœ— æ•°æ®ç±»å‹ {data_type} ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
            logger.info(f"å¯ç”¨çš„æ•°æ®ç±»å‹: {', '.join(self.enabled_sources.keys())}")
            return False
        
        if data_source not in self.enabled_sources[data_type]:
            logger.error(f"âœ— æ•°æ®æº {data_source} åœ¨ {data_type} ä¸­ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
            logger.info(f"å¯ç”¨çš„æ•°æ®æº: {', '.join(self.enabled_sources[data_type])}")
            return False
        
        try:
            # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
            files_info = f"å‰{max_files}ä¸ªæ–‡ä»¶" if max_files else "å…¨éƒ¨æ–‡ä»¶"
            logger.info(f"å¤„ç†èŒƒå›´: {files_info}")
            
            # æ‰§è¡Œæ¸…æ´—
            result_df = self.service.clean_data_by_pipeline(
                pipeline_name=data_type,
                data_source=data_source,
                max_files=max_files
            )
            
            if result_df is not None and not result_df.empty:
                # ä¿å­˜æ•°æ®
                file_path = self.service.save_cleaned_data(
                    df=result_df,
                    pipeline_name=data_type,
                    data_source=data_source
                )
                
                if file_path:
                    logger.info(f"âœ“ æ•°æ®æ¸…æ´—æˆåŠŸ")
                    logger.info(f"  æ•°æ®å½¢çŠ¶: {result_df.shape}")
                    logger.info(f"  è¾“å‡ºæ–‡ä»¶: {file_path}")
                    
                    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¤§å°
                    if os.path.exists(file_path):
                        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                        logger.info(f"  æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
                    
                    logger.info(f"  åˆ—å: {list(result_df.columns)[:10]}{'...' if len(result_df.columns) > 10 else ''}")
                    return True
                else:
                    logger.error(f"âœ— æ•°æ®ä¿å­˜å¤±è´¥")
                    return False
            else:
                logger.error(f"âœ— æ¸…æ´—åæ•°æ®ä¸ºç©º")
                return False
                
        except Exception as e:
            logger.error(f"âœ— æ¸…æ´—æ•°æ®æºå¤±è´¥: {str(e)}")
            return False
    
    def clean_all_data_sources_in_type(self, data_type: str, max_files: Optional[int] = None) -> Dict[str, bool]:
        """æ¸…æ´—æŒ‡å®šæ•°æ®ç±»å‹ä¸‹çš„æ‰€æœ‰æ•°æ®æº"""
        logger.info(f"=== å¼€å§‹æ¸…æ´—æ•°æ®ç±»å‹: {data_type} ===")
        
        if data_type not in self.enabled_sources:
            logger.error(f"âœ— æ•°æ®ç±»å‹ {data_type} ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
            return {}
        
        sources = self.enabled_sources[data_type]
        results = {}
        
        logger.info(f"å°†è¦å¤„ç† {len(sources)} ä¸ªæ•°æ®æº:")
        for source in sources:
            logger.info(f"  âœ“ {source}")
        
        for data_source in sources:
            logger.info(f"\n--- å¤„ç†æ•°æ®æº: {data_source} ---")
            success = self.clean_specific_data_source(data_type, data_source, max_files)
            results[data_source] = success
        
        # è¾“å‡ºæ±‡æ€»ç»“æœ
        logger.info(f"\n=== {data_type} æ¸…æ´—ç»“æœæ±‡æ€» ===")
        success_count = sum(1 for success in results.values() if success)
        total_count = len(results)
        
        for source, success in results.items():
            status = "âœ“ æˆåŠŸ" if success else "âœ— å¤±è´¥"
            logger.info(f"  {source}: {status}")
        
        logger.info(f"\næ€»è®¡: æˆåŠŸ {success_count}/{total_count} ä¸ªæ•°æ®æº")
        return results
    
    def clean_all_enabled_data_sources(self, max_files: Optional[int] = None) -> Dict[str, Dict[str, bool]]:
        """æ¸…æ´—æ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"""
        files_info = f"å‰{max_files}ä¸ªæ–‡ä»¶" if max_files else "å…¨éƒ¨æ–‡ä»¶"
        logger.info(f"\n{'='*60}")
        logger.info(f"å¼€å§‹æ¸…æ´—æ‰€æœ‰å¯ç”¨çš„æ•°æ®æº - å¤„ç†{files_info}")
        logger.info(f"{'='*60}")
        
        if not self.enabled_sources:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®æº")
            return {}
        
        # æ˜¾ç¤ºå°†è¦å¤„ç†çš„æ•°æ®æº
        self.list_available_data_sources()
        logger.info(f"{'='*60}")
        
        all_results = {}
        
        # æŒ‰æ•°æ®ç±»å‹å¤„ç†
        for data_type in self.enabled_sources.keys():
            type_results = self.clean_all_data_sources_in_type(data_type, max_files)
            all_results[data_type] = type_results
        
        # æ‰“å°æ€»ä½“æ±‡æ€»ç»“æœ
        logger.info(f"\n{'='*60}")
        logger.info("æ€»ä½“æ¸…æ´—ç»“æœæ±‡æ€»:")
        logger.info(f"{'='*60}")
        
        total_success = 0
        total_failed = 0
        
        for data_type, type_results in all_results.items():
            logger.info(f"\nğŸ“Š {data_type}:")
            for source, success in type_results.items():
                if success:
                    logger.info(f"  âœ“ {source}: æˆåŠŸ")
                    total_success += 1
                else:
                    logger.info(f"  âœ— {source}: å¤±è´¥")
                    total_failed += 1
        
        logger.info(f"\n{'='*60}")
        logger.info(f"æ€»è®¡: æˆåŠŸ {total_success} ä¸ª, å¤±è´¥ {total_failed} ä¸ª")
        logger.info(f"{'='*60}")
        
        return all_results


def run_specific_data_source(data_type: str = None, data_source: str = None, max_files: Optional[int] = None):
    """è¿è¡ŒæŒ‡å®šçš„æ•°æ®æºæ¸…æ´—"""
    example = DataCleaningServiceExample()
    
    if data_type is None:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œåˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®æº
        example.list_available_data_sources()
        
        # é»˜è®¤è¿è¡Œdividendæ•°æ®æº
        data_type = 'dividend'
        data_source = 'dividend'
        logger.info(f"\né»˜è®¤è¿è¡Œ: {data_type}/{data_source}")
    
    if data_source is None:
        # å¦‚æœåªæŒ‡å®šäº†æ•°æ®ç±»å‹ï¼Œæ¸…æ´—è¯¥ç±»å‹ä¸‹çš„æ‰€æœ‰æ•°æ®æº
        logger.info(f"=== å¼€å§‹è¿è¡Œæ•°æ®ç±»å‹: {data_type} ===")
        
        try:
            results = example.clean_all_data_sources_in_type(data_type, max_files)
            
            success_count = sum(1 for success in results.values() if success)
            total_count = len(results)
            
            if success_count == total_count:
                logger.info(f"\nğŸ¯ {data_type} æ¸…æ´—å®Œæˆ! æ‰€æœ‰ {total_count} ä¸ªæ•°æ®æºéƒ½æˆåŠŸ")
                logger.info("ğŸ’¡ è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆï¼Œå¯ä»¥æŸ¥çœ‹ç»“æœ")
                return True
            else:
                logger.error(f"\nâš ï¸  {data_type} æ¸…æ´—å®Œæˆï¼Œä½†æœ‰ {total_count - success_count} ä¸ªæ•°æ®æºå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"{data_type} æ¸…æ´—å¼‚å¸¸: {e}")
            return False
    else:
        # æ¸…æ´—æŒ‡å®šçš„æ•°æ®æº
        logger.info(f"=== å¼€å§‹è¿è¡Œæ•°æ®æº: {data_type}/{data_source} ===")
        
        try:
            success = example.clean_specific_data_source(data_type, data_source, max_files)
            
            if success:
                logger.info(f"\nğŸ¯ {data_type}/{data_source} æ¸…æ´—æˆåŠŸ!")
                logger.info("ğŸ’¡ è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆï¼Œå¯ä»¥æŸ¥çœ‹ç»“æœ")
            else:
                logger.error(f"\nâš ï¸  {data_type}/{data_source} æ¸…æ´—å¤±è´¥")
                
            return success
                
        except Exception as e:
            logger.error(f"{data_type}/{data_source} æ¸…æ´—å¼‚å¸¸: {e}")
            return False


def run_all_enabled_data_sources(max_files: Optional[int] = None):
    """è¿è¡Œæ‰€æœ‰å¯ç”¨çš„æ•°æ®æºæ¸…æ´—"""
    logger.info("=== å¼€å§‹è¿è¡Œæ‰€æœ‰å¯ç”¨æ•°æ®æºæ¸…æ´— ===")
    
    try:
        example = DataCleaningServiceExample()
        results = example.clean_all_enabled_data_sources(max_files)
        
        # è®¡ç®—æ€»ä½“æˆåŠŸç‡
        total_success = 0
        total_count = 0
        
        for type_results in results.values():
            for success in type_results.values():
                if success:
                    total_success += 1
                total_count += 1
        
        if total_success == total_count:
            logger.info(f"\nğŸ¯ æ‰€æœ‰æ•°æ®æºæ¸…æ´—æˆåŠŸ! å…±å¤„ç† {total_count} ä¸ªæ•°æ®æº")
            logger.info("ğŸ’¡ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆï¼Œå¯ä»¥æŸ¥çœ‹ç»“æœ")
            return True
        else:
            logger.error(f"\nâš ï¸  æ•°æ®æºæ¸…æ´—å®Œæˆï¼ŒæˆåŠŸ {total_success}/{total_count} ä¸ª")
            return False
            
    except Exception as e:
        logger.error(f"æ¸…æ´—æ‰€æœ‰æ•°æ®æºå¼‚å¸¸: {e}")
        return False


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == "--all":
            # è¿è¡Œæ‰€æœ‰å¯ç”¨çš„æ•°æ®æºæ¸…æ´—
            max_files = None
            if len(sys.argv) > 2:
                try:
                    max_files = int(sys.argv[2])
                except ValueError:
                    if sys.argv[2].lower() != 'all':
                        logger.error("æ–‡ä»¶æ•°é‡å‚æ•°é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—æˆ–'all'")
                        return
            
            run_all_enabled_data_sources(max_files)
            
        elif sys.argv[1] == "--list":
            # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®æº
            example = DataCleaningServiceExample()
            example.list_available_data_sources()
            
        elif sys.argv[1] == "--info":
            # è·å–æ•°æ®æºä¿¡æ¯
            if len(sys.argv) < 3:
                logger.error("è¯·æŒ‡å®šæ•°æ®ç±»å‹: --info <data_type> [data_source]")
                return
            
            data_type = sys.argv[2]
            data_source = sys.argv[3] if len(sys.argv) > 3 else None
            
            example = DataCleaningServiceExample()
            info = example.get_data_source_info(data_type, data_source)
            
            if info:
                logger.info(f"æ•°æ®ç±»å‹: {info['data_type']}")
                logger.info(f"æè¿°: {info['description']}")
                logger.info(f"è¾“å‡ºè·¯å¾„: {info['output_path']}")
                logger.info(f"å¯ç”¨æ•°æ®æº: {', '.join(info['available_sources'])}")
                
                if 'data_source' in info:
                    logger.info(f"æ•°æ®æº: {info['data_source']}")
                    logger.info(f"å¯ç”¨çŠ¶æ€: {info['enabled']}")
                    logger.info(f"æµæ°´çº¿æ­¥éª¤æ•°: {info['pipeline_steps']}")
            
        else:
            # è§£ææ•°æ®ç±»å‹å’Œæ•°æ®æº
            parts = sys.argv[1].split('/')
            data_type = parts[0]
            data_source = parts[1] if len(parts) > 1 else None
            
            # è§£ææ–‡ä»¶æ•°é‡å‚æ•°
            max_files = None
            if len(sys.argv) > 2:
                try:
                    max_files = int(sys.argv[2])
                except ValueError:
                    if sys.argv[2].lower() != 'all':
                        logger.error("æ–‡ä»¶æ•°é‡å‚æ•°é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—æˆ–'all'")
                        return
            
            run_specific_data_source(data_type, data_source, max_files)
    else:
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        logger.info("ğŸ’¡ æ•°æ®æ¸…æ´—æœåŠ¡ä½¿ç”¨è¯´æ˜:")
        logger.info("  python example_data_cleaning_service.py                           # é»˜è®¤è¿è¡Œdividend/dividend")
        logger.info("  python example_data_cleaning_service.py dividend                 # è¿è¡Œdividendç±»å‹ä¸‹æ‰€æœ‰æ•°æ®æº")
        logger.info("  python example_data_cleaning_service.py dividend/dividend       # è¿è¡ŒæŒ‡å®šæ•°æ®æº")
        logger.info("  python example_data_cleaning_service.py dividend/dividend 10    # è¿è¡ŒæŒ‡å®šæ•°æ®æºï¼Œå¤„ç†å‰10ä¸ªæ–‡ä»¶")
        logger.info("  python example_data_cleaning_service.py --all                   # è¿è¡Œæ‰€æœ‰å¯ç”¨çš„æ•°æ®æº")
        logger.info("  python example_data_cleaning_service.py --all 10                # è¿è¡Œæ‰€æœ‰å¯ç”¨çš„æ•°æ®æºï¼Œæ¯ä¸ªå¤„ç†å‰10ä¸ªæ–‡ä»¶")
        logger.info("  python example_data_cleaning_service.py --list                  # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®æº")
        logger.info("  python example_data_cleaning_service.py --info dividend         # è·å–æ•°æ®ç±»å‹ä¿¡æ¯")
        logger.info("  python example_data_cleaning_service.py --info dividend dividend # è·å–æ•°æ®æºè¯¦ç»†ä¿¡æ¯")
        logger.info("")
        
        # é»˜è®¤è¿è¡Œdividendæ•°æ®æº
        run_specific_data_source()


if __name__ == "__main__":
    main()
